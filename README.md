# FreshTunes
## CS 410 Final Project, Ben Svoboda
### 3 credit, undergraduate section

### Abstract/Proposal
FreshTunes aims to be a content-based recommender system that uses the Spotify API to generate a playlist of recommendations based off an existing user-curated playlist. Spotipy is a python library for the Spotify API that can be used to extract basic information from songs such as the artist, popularity, and release date. However, there are many more factors that go into creating a spotify playlist. Spotify playlists are often used to represent a certain "mood" or "feeling", which will is why the advanced audio features such as danceability, key, tempo & tempo will be integral to creating an effective playlist recommender system. The user will be allowed to input their playlist link & the desired output playlist length (x). Then, based on the audio features of the input playlist, the x most similar songs will be returned. Me and my friends are always looking for means of discovering  new music, especially when it's each other's. This app was created to aid us in our quest to expand our musical tastes & preferences.  


# Documentation
## 1. Function Overview
### generateCSVs.ipynb (data acquisition)
- Spotify developer authentication is performed
- ```def userToPlaylistList(username):``` allows user to input a username & returns a list of their playlist links
- ```def playlistData(url,offset=0):``` allows user to input a playlist url & offset, which reads, analyzes & compiles tracks  in the playlist, saving each 100 tracks along with track metadata & audio features into separate csvs.
- Then, all of my friends' spotify usernames were compiled & fed into ```userToPlaylistList()```. The results were concatenated into a list of every playlist & ```playlistData()``` was run on each playlist entry.
### recommender.ipynb (data processing & recommender system)
- ```def csvCondensor(directory):``` takes a data directory name as input & reads each user csv in that directory created with generateCSVs.ipynb. These csvs are then read and concatenated with each other as well as the large, base, csv "downloadedData.csv". This aggregate csv is then returned.
- **Evaluation:** The evaulation step reads three sample playlists that represent different describing words as dataframes. Then, by use of a cosine similarity matrix, the 15 rows (songs) of the aggregate dataframe with the highest similarity scores to each playlist are returned. From there, the user listened to each of the songs returned & evaluated which fit the playlist description & which didn't. Accuracy scores were then derived based on these evaluations.
- ```def usingYourPlaylist(link,offset=0,length=20):``` Allows user to input their playlist link,what song in their playlist to start reading & the output playlist's length. This computes audio features & concatenates them in a dataframe for each song in the input playlist. Cosine similarity is performed with the resulting dataframe and the aggregate matrix generated by the ```csvCondensor(directory)``` function. The rows of the aggregate dataframe with the highest similarity score to the playlist are returned, which serves as the user's new music recommendation.
## 2. Code Implementation
### generateCSVs.ipynb
- Spotify developer authentication is performed by loading a file of keys & inputting the corresponding clientId & clientSecret into the Spotipy auth ```SpotifyClientCredentials()``` function
- ```def userToPlaylistList(username):``` uses Spotipy function ```user_playlists()``` to generate a list of playlist links from a username.
- ```def playlistData(url,offset=0):``` uses Spotipy function ```playlist_tracks(query)["items"]``` to query a list of track dictionaries from the playlist link. A limit of 100 tracks are queried at a time to avoid being rate limited. A loop is used to iterate through each track dictionary in the list with a valid Spoitify link. Each track dictionary is indexed so the corresponding dictionary so track name, artist name & url are extracted. The audio features are also extracted by applying Spotipy's ```audio_features([url]) ``` to the track url. The extracted metadata & audio song values are appended as rows to a dataframe that's saved as a new CSV & reset for every 100 songs handled.
- Finally, ```userToPlaylistList(username)``` is run over every element of a list of me & my friends' usernames with the results concatenated into a large list of playlist links.  ```playlistData(url,offset=0):``` is applied to each list entry to ultimately convert all of the users' songs into a csv row in the data folder.
### recommender.ipynb 
- ```def csvCondensor(directory):``` Reads the csvs in the input directory that correspond to those generated by ```playlistData()``` in generateCSVs.ipynb (those where ```startswith(good) == true```). For each read csv, there were two "index columns". One of which is dropped before appending the resulting dataframe to a list of dataframes. Once all of the CSVs were processed and appended, ```pd.concat(alldfs)```, was applied to the resultant list of dataframes to combine them into one. A (big) 130,000 song csv of audio features that was downloaded from the internet was read in as a dataframe at this time. It had track ids instead of links and ```bigCsv["url"] = "https://open.spotify.com/track/"+bigCsv["url"]``` is used to convert the track ids to links. The downloaded csv also had features that weren't necessary for this project, so the list difference is taken between the downloaded data columns & user generated data columns. The resultant list of unecessary features is dropped with ```bigCsv = bigCsv.drop(listDifference,axis=1)```. At this point in the code, the two dataframes have the same set of features, so ```pd.concat()``` was implemented again to combine them into one dataframe to be used as a de facto song database. All of the artist/song names of the resultant dataframe are lowered, so that capitalization isn't a factor preventing duplicates songs from being removed. Only the first of duplicate song urls is kept. The same occurs when multiple rows with the same artist & song values are present. These operations are performed sequentially with the following code: ```    result = result.drop_duplicates(subset='url', keep='first') 
   result = result.drop_duplicates(subset=['name','artist'], keep='first')```  
   The resulting dataframe is reindexed with ```result.reset_index(drop=True)``` so each row of the dataframe is numbered in sequential order before being returned.


- **Evaluation:** First ```pd.read_csv``` is implemented to read in the three dataframes to be evaluated. Then the following procedure was applied to each of these dataframes:
    1. ```scaler = StandardScaler()``` is used to create a scaler
    2. This scaler is used to transform both the large "database" dataframe (condensed) and the playlist dataframe (toEvaluate) into standard coordinates by ```dataScaled =   scaler.fit_transform(condensed[scalable_columns])
    playlistScaled = scaler.transform(toEvaluate[scalable_columns])```. scalable_columns is a list of all of the columns names corresponding to (numerical) audio features.
    3. The cosine similarity matrix of the playlist & "database" dataframes is then generated for which the mean of each feature is taken with the following code: ```similarity_matrix = cosine_similarity(playlistScaled,dataScaled)
    similarity_scores = similarity_matrix.mean(axis=0)```
    4. The similarity scores list is added as a column to the "database" matrix, which is then sorted by similarity_scores with ```recommendations = conSim.sort_values(by='similarity_score', ascending=False)``` before returning the top 15 recommendations with ```recommendations.head(15)```
    5. The returned songs were listened to. Each of the three  playlists has a description of a word or two. Playlists described by two words were categorized as either fitting one words or the other, fitting none of the words or fitting both of the words. The accuracy was calculated for each describing word as well as the "Overall accuracy" by dividing the number of returned songs fitting both description words by the total number of songs in the returned playlist (15). Only "Overall Accuracy" was calculated for the playlist with a single describing words.
    
- A similar Spotify developer authentication process is performed. However, instead of loading keys from json into ```SpotifyClientCredentials()```, the ClientId/ClientSecret are already provided for a dummy spotify account.
- ```def usingYourPlaylist(link,offset=0,length=20):``` This function contains recycled snippets from the rest of the code base; there is no code that was not explained previously in this function.

## 3. Usage instructions
1. Clone the repository locally
2. Run ``` pip install -r requirements.txt ``` to install Spotipy, Pandas & scikit-learn
3. Double click on data.zip which will be your local FreshTunes folder. This will decompress the data that is used for recommendation playlists
4. Open recommender.ipynb In the last line of the last cell change the link to your playlist & then run all of the notebook's cells
5. A recommended playlist based off yours will be output below the final cell
6. Notes: there is no need to provide your own Spotify developer credentials, because a dummy account was made for the purpose of presenting. Also, beware of rate limiting, and try not to run the "usingYourPlaylist()" function  aggressively.
